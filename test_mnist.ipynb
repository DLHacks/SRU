{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNISTで比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "torch.cuda.set_device(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MNISTデータセット準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    mnist = fetch_mldata('MNIST original')\n",
    "    mnist_X, mnist_y = shuffle(mnist.data, mnist.target, random_state=42)\n",
    "    mnist_X = mnist_X / 255.0\n",
    "\n",
    "    # pytorch用に型変換\n",
    "    mnist_X, mnist_y = mnist_X.astype('float32'), mnist_y.astype('int64')\n",
    "\n",
    "    # 2次元の画像を、各行を互い違いにして1次元に変換\n",
    "    def flatten_img(images):\n",
    "        '''\n",
    "        images: shape => (n, rows, columns)\n",
    "        output: shape => (n, rows*columns)\n",
    "        '''\n",
    "        n_rows    = images.shape[1]\n",
    "        n_columns = images.shape[2]\n",
    "        for num in range(n_rows):\n",
    "            if num % 2 != 0:\n",
    "                images[:, num, :] = images[:, num, :][:, ::-1]\n",
    "        output = images.reshape(-1, n_rows*n_columns)\n",
    "        return output\n",
    "\n",
    "    mnist_X = mnist_X.reshape(-1, 28, 28)\n",
    "#     mnist_X = flatten_img(mnist_X)\n",
    "#     # X.shape => (n_samples, seq_len, n_features) に変換\n",
    "#     mnist_X = mnist_X[:, :, np.newaxis]\n",
    "\n",
    "    # 訓練、テスト、検証データに分割\n",
    "    train_X, test_X, train_y, test_y = train_test_split(mnist_X, mnist_y,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=42)\n",
    "    train_X, valid_X, train_y, valid_y = train_test_split(train_X, train_y,\n",
    "                                                          test_size=0.1,\n",
    "                                                          random_state=42)\n",
    "\n",
    "    return train_X, test_X, train_y, test_y, valid_X, valid_y\n",
    "\n",
    "train_X, test_X, train_y, test_y, valid_X, valid_y = load_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. モデル構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from gru import GRU\n",
    "from lstm import LSTM\n",
    "from sru import SRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 訓練の準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import torch.optim as optim\n",
    "\n",
    "# 計算時間を表示させる\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "# batchあたりの訓練\n",
    "def train(model, inputs, labels, optimizer, criterion, clip):\n",
    "    batch_size = inputs.size(1)\n",
    "    # 隠れ変数の初期化\n",
    "    model.initHidden(batch_size)\n",
    "    # 勾配の初期化\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    accuracy = (torch.max(outputs, 1)[1] == labels).sum().data[0] / batch_size\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "    optimizer.step()\n",
    "    return loss.data[0], accuracy\n",
    "\n",
    "\n",
    "# 検証\n",
    "def validate(model, inputs, labels, optimizer, criterion):\n",
    "    # 隠れ変数の初期化\n",
    "    batch_size = inputs.size(1)\n",
    "    model.initHidden(batch_size)\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    accuracy = (torch.max(outputs, 1)[1] == labels).sum().data[0] / batch_size\n",
    "    return loss.data[0], accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. パラメータの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = train_X.shape[2]\n",
    "output_size = np.unique(train_y).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' GRU '''\n",
    "hidden_size = 100\n",
    "dropout = 0.2\n",
    "lr = 0.05\n",
    "lr_decay = 0.99\n",
    "init_forget_bias = 1\n",
    "clip = 1\n",
    "\n",
    "# インスタンスの作成\n",
    "model = GRU(input_size, hidden_size, output_size, dropout=dropout)\n",
    "model.initWeight(init_forget_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' LSTM '''\n",
    "hidden_size = 100\n",
    "dropout = 0.2\n",
    "lr = 0.01\n",
    "lr_decay = 1\n",
    "init_forget_bias = 1\n",
    "clip = 100\n",
    "\n",
    "# インスタンスの作成\n",
    "model = LSTM(input_size, hidden_size, output_size, dropout=dropout)\n",
    "model.initWeight(init_forget_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' SRU '''\n",
    "phi_size = 200\n",
    "r_size = 60\n",
    "cell_out_size = 200\n",
    "lr = 0.1\n",
    "lr_decay = 0.99\n",
    "clip = 1\n",
    "\n",
    "torch.cuda.manual_seed(0)\n",
    "model = SRU(input_size, phi_size, r_size, cell_out_size,\n",
    "            output_size, A=[0, 0.5, 0.9, 0.99, 0.999],\n",
    "            dropout=0.2)\n",
    "model.initWeight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' loss, optimizerの定義 '''\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=lr) # SRUはRMSprop等と相性が悪い?\n",
    "# 10ステップごとに学習率をdecayさせる\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:: 1, (0m 1s) train_cost: 1.881, valid_cost: 0.777, train_acc: 0.131, valid_acc: 0.221\n",
      "EPOCH:: 2, (0m 4s) train_cost: 0.489, valid_cost: 0.421, train_acc: 0.124, valid_acc: 0.118\n",
      "EPOCH:: 3, (0m 6s) train_cost: 0.228, valid_cost: 0.717, train_acc: 0.183, valid_acc: 0.054\n",
      "EPOCH:: 4, (0m 8s) train_cost: 0.398, valid_cost: 0.137, train_acc: 0.179, valid_acc: 0.209\n",
      "EPOCH:: 5, (0m 10s) train_cost: 0.127, valid_cost: 0.114, train_acc: 0.213, valid_acc: 0.216\n",
      "EPOCH:: 6, (0m 12s) train_cost: 0.110, valid_cost: 0.097, train_acc: 0.218, valid_acc: 0.219\n",
      "EPOCH:: 7, (0m 14s) train_cost: 0.166, valid_cost: 0.088, train_acc: 0.201, valid_acc: 0.223\n",
      "EPOCH:: 8, (0m 16s) train_cost: 0.071, valid_cost: 0.079, train_acc: 0.229, valid_acc: 0.226\n",
      "EPOCH:: 9, (0m 17s) train_cost: 0.062, valid_cost: 0.088, train_acc: 0.231, valid_acc: 0.224\n",
      "EPOCH:: 10, (0m 19s) train_cost: 0.056, valid_cost: 0.068, train_acc: 0.233, valid_acc: 0.231\n",
      "EPOCH:: 11, (0m 21s) train_cost: 0.050, valid_cost: 0.068, train_acc: 0.235, valid_acc: 0.232\n",
      "EPOCH:: 12, (0m 23s) train_cost: 0.051, valid_cost: 0.064, train_acc: 0.234, valid_acc: 0.232\n",
      "EPOCH:: 13, (0m 25s) train_cost: 0.229, valid_cost: 0.074, train_acc: 0.215, valid_acc: 0.228\n",
      "EPOCH:: 14, (0m 27s) train_cost: 0.052, valid_cost: 0.053, train_acc: 0.235, valid_acc: 0.234\n",
      "EPOCH:: 15, (0m 29s) train_cost: 0.035, valid_cost: 0.056, train_acc: 0.239, valid_acc: 0.233\n",
      "EPOCH:: 16, (0m 30s) train_cost: 0.028, valid_cost: 0.048, train_acc: 0.242, valid_acc: 0.235\n",
      "EPOCH:: 17, (0m 32s) train_cost: 0.030, valid_cost: 0.055, train_acc: 0.241, valid_acc: 0.236\n",
      "EPOCH:: 18, (0m 34s) train_cost: 0.233, valid_cost: 0.092, train_acc: 0.213, valid_acc: 0.223\n",
      "EPOCH:: 19, (0m 36s) train_cost: 0.054, valid_cost: 0.055, train_acc: 0.235, valid_acc: 0.234\n",
      "EPOCH:: 20, (0m 38s) train_cost: 0.029, valid_cost: 0.058, train_acc: 0.242, valid_acc: 0.233\n",
      "EPOCH:: 21, (0m 40s) train_cost: 0.022, valid_cost: 0.052, train_acc: 0.244, valid_acc: 0.236\n",
      "EPOCH:: 22, (0m 41s) train_cost: 0.021, valid_cost: 0.052, train_acc: 0.244, valid_acc: 0.236\n",
      "EPOCH:: 23, (0m 43s) train_cost: 0.026, valid_cost: 0.079, train_acc: 0.242, valid_acc: 0.229\n",
      "EPOCH:: 24, (0m 45s) train_cost: 0.024, valid_cost: 0.055, train_acc: 0.242, valid_acc: 0.237\n",
      "EPOCH:: 25, (0m 46s) train_cost: 0.024, valid_cost: 0.048, train_acc: 0.242, valid_acc: 0.238\n",
      "EPOCH:: 26, (0m 48s) train_cost: 0.016, valid_cost: 0.109, train_acc: 0.240, valid_acc: 0.221\n",
      "EPOCH:: 27, (0m 50s) train_cost: 0.030, valid_cost: 0.054, train_acc: 0.241, valid_acc: 0.236\n",
      "EPOCH:: 28, (0m 52s) train_cost: 0.048, valid_cost: 0.054, train_acc: 0.237, valid_acc: 0.235\n",
      "EPOCH:: 29, (0m 54s) train_cost: 0.017, valid_cost: 0.051, train_acc: 0.240, valid_acc: 0.237\n",
      "EPOCH:: 30, (0m 56s) train_cost: 0.016, valid_cost: 0.052, train_acc: 0.230, valid_acc: 0.236\n",
      "EPOCH:: 31, (0m 57s) train_cost: 0.015, valid_cost: 0.055, train_acc: 0.246, valid_acc: 0.236\n",
      "EPOCH:: 32, (0m 59s) train_cost: 0.017, valid_cost: 0.043, train_acc: 0.240, valid_acc: 0.238\n",
      "EPOCH:: 33, (1m 1s) train_cost: 0.016, valid_cost: 0.064, train_acc: 0.240, valid_acc: 0.234\n",
      "EPOCH:: 34, (1m 2s) train_cost: 0.021, valid_cost: 0.060, train_acc: 0.244, valid_acc: 0.236\n",
      "EPOCH:: 35, (1m 3s) train_cost: 0.014, valid_cost: 0.060, train_acc: 0.246, valid_acc: 0.237\n",
      "EPOCH:: 36, (1m 4s) train_cost: 0.061, valid_cost: 0.055, train_acc: 0.234, valid_acc: 0.236\n",
      "EPOCH:: 37, (1m 5s) train_cost: 0.010, valid_cost: 0.051, train_acc: 0.242, valid_acc: 0.237\n",
      "EPOCH:: 38, (1m 6s) train_cost: 0.005, valid_cost: 0.052, train_acc: 0.187, valid_acc: 0.238\n",
      "EPOCH:: 39, (1m 8s) train_cost: 0.014, valid_cost: 0.060, train_acc: 0.185, valid_acc: 0.236\n",
      "EPOCH:: 40, (1m 9s) train_cost: 0.019, valid_cost: 0.058, train_acc: 0.244, valid_acc: 0.234\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "''' 訓練 '''\n",
    "n_epochs = 40\n",
    "batch_size = 1024\n",
    "n_batches = train_X.shape[0]//batch_size\n",
    "n_batches_v = valid_X.shape[0]//batch_size\n",
    "all_acc = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_cost, valid_cost, train_acc, valid_acc  = 0, 0, 0, 0\n",
    "    train_X, train_y = shuffle(train_X, train_y, random_state=42)\n",
    "\n",
    "    # 訓練\n",
    "    model.train()\n",
    "    train_X_t = np.transpose(train_X, (1, 0, 2)) # X.shape => (seq_len, n_samples, n_features) に変換\n",
    "    for i in range(n_batches):\n",
    "        scheduler.step()\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        inputs, labels = train_X_t[:, start:end, :], train_y[start:end]\n",
    "        inputs, labels = Variable(torch.from_numpy(inputs).cuda()\n",
    "                         ), Variable(torch.from_numpy(labels).cuda())\n",
    "        cost, accuracy = train(model, inputs, labels, optimizer, criterion, clip)\n",
    "        train_cost += cost / n_batches\n",
    "        train_acc  += accuracy / n_batches\n",
    "\n",
    "    # 検証\n",
    "    model.eval()\n",
    "    valid_X_t = np.transpose(valid_X, (1, 0, 2)) # X.shape => (seq_len, n_samples, n_features) に変換\n",
    "    for i in range(n_batches_v):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        inputs, labels = valid_X_t[:, start:end, :], valid_y[start:end]\n",
    "        inputs, labels = Variable(torch.from_numpy(inputs).cuda()\n",
    "                         ), Variable(torch.from_numpy(labels).cuda())\n",
    "        cost, accuracy = validate(model, inputs, labels, optimizer, criterion)\n",
    "        valid_cost += cost / n_batches_v\n",
    "        valid_acc += accuracy / n_batches_v\n",
    "\n",
    "    all_acc.append(valid_acc)\n",
    "    print('EPOCH:: %i, (%s) train_cost: %.3f, valid_cost: %.3f, train_acc: %.3f, valid_acc: %.3f' % (epoch + 1,\n",
    "                       timeSince(start_time), train_cost, valid_cost, train_acc, valid_acc))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sand Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 236\n",
       "[torch.cuda.ByteTensor of size 1 (GPU 2)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' テスト '''\n",
    "test_X_t = np.transpose(test_X[5000:5500], (1, 0, 2))\n",
    "inputs, labels = test_X_t, test_y[5000:5500]\n",
    "inputs, labels = Variable(torch.from_numpy(inputs).cuda()\n",
    "                 ), Variable(torch.from_numpy(labels).cuda())\n",
    "model.initHidden(inputs.size(1))\n",
    "outputs = model(inputs)\n",
    "\n",
    "# 正解数\n",
    "(torch.max(outputs, 1)[1] == labels).sum()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 勾配の確認 '''\n",
    "loss = criterion(outputs, labels)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-03 *\n",
       " -1.3640\n",
       " -3.9899\n",
       "  3.9046\n",
       " -0.2410\n",
       "  2.9663\n",
       " -7.2542\n",
       "  1.3611\n",
       "  3.0439\n",
       " -0.1566\n",
       "  1.7300\n",
       "[torch.cuda.FloatTensor of size 10 (GPU 2)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[5].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-1.6677e+00 -2.1592e+00  1.1355e+01  ...  -3.4879e+00 -1.7871e-01 -2.5049e+00\n",
       "-6.0168e+00  1.2347e+01 -1.9971e+00  ...  -9.8720e-01 -2.5509e+00 -2.5539e+00\n",
       "-4.8645e+00  1.1795e+01 -5.2879e+00  ...  -4.3604e-01 -6.9174e-01 -2.8169e+00\n",
       "                ...                   ⋱                   ...                \n",
       "-5.4314e+00 -9.9449e-01 -1.3796e+00  ...   1.0591e+01 -1.0177e+00  3.4657e-01\n",
       "-3.5827e-02 -4.4396e-01 -2.2040e+00  ...  -7.9230e+00 -1.8895e+00 -4.1059e+00\n",
       " 6.9418e-01 -6.3189e-01 -1.8002e+00  ...  -8.9616e+00 -1.7312e+00 -4.9029e+00\n",
       "[torch.cuda.FloatTensor of size 500x10 (GPU 2)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "outputsが全てのサンプルで同じになる理由: XWが0, bが≠0\n",
    "\"\"\"\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_py35",
   "language": "python",
   "name": "tf_py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
