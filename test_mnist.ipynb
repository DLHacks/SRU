{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNISTで比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "torch.cuda.set_device(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MNISTデータセット準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    mnist = fetch_mldata('MNIST original')\n",
    "    mnist_X, mnist_y = shuffle(mnist.data, mnist.target, random_state=42)\n",
    "    mnist_X = mnist_X / 255.0\n",
    "\n",
    "    # pytorch用に型変換\n",
    "    mnist_X, mnist_y = mnist_X.astype('float32'), mnist_y.astype('int64')\n",
    "\n",
    "    # 2次元の画像を、各行を互い違いにして1次元に変換\n",
    "    def flatten_img(images):\n",
    "        '''\n",
    "        images: shape => (n, rows, columns)\n",
    "        output: shape => (n, rows*columns)\n",
    "        '''\n",
    "        n_rows    = images.shape[1]\n",
    "        n_columns = images.shape[2]\n",
    "        for num in range(n_rows):\n",
    "            if num % 2 != 0:\n",
    "                images[:, num, :] = images[:, num, :][:, ::-1]\n",
    "        output = images.reshape(-1, n_rows*n_columns)\n",
    "        return output\n",
    "\n",
    "    mnist_X = mnist_X.reshape(-1, 28, 28)\n",
    "    mnist_X = flatten_img(mnist_X) # X.shape => (n_samples, seq_len) \n",
    "    mnist_X = mnist_X[:, :, np.newaxis] # X.shape => (n_samples, seq_len, n_features) \n",
    "\n",
    "    # 訓練、テスト、検証データに分割\n",
    "    train_X, test_X, train_y, test_y = train_test_split(mnist_X, mnist_y,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=42)\n",
    "    train_X, valid_X, train_y, valid_y = train_test_split(train_X, train_y,\n",
    "                                                          test_size=0.1,\n",
    "                                                          random_state=42)\n",
    "\n",
    "    return train_X, test_X, train_y, test_y, valid_X, valid_y\n",
    "\n",
    "train_X, test_X, train_y, test_y, valid_X, valid_y = load_mnist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. モデル構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from models import SRU, GRU, LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 訓練の準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import torch.optim as optim\n",
    "\n",
    "# 計算時間を表示させる\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "# batchあたりの訓練\n",
    "def train(model, inputs, labels, optimizer, criterion, clip):\n",
    "    batch_size = inputs.size(1)\n",
    "    # 隠れ変数の初期化\n",
    "    model.initHidden(batch_size)\n",
    "    # 勾配の初期化\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    accuracy = (torch.max(outputs, 1)[1] == labels).sum().data[0] / batch_size\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "    optimizer.step()\n",
    "    return loss.data[0], accuracy\n",
    "\n",
    "\n",
    "# 検証\n",
    "def validate(model, inputs, labels, optimizer, criterion):\n",
    "    # 隠れ変数の初期化\n",
    "    batch_size = inputs.size(1)\n",
    "    model.initHidden(batch_size)\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    accuracy = (torch.max(outputs, 1)[1] == labels).sum().data[0] / batch_size\n",
    "    return loss.data[0], accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. パラメータの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = train_X.shape[2]\n",
    "output_size = np.unique(train_y).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' GRU '''\n",
    "hidden_size = 100\n",
    "dropout = 0.2\n",
    "lr = 0.05\n",
    "lr_decay = 0.99\n",
    "init_forget_bias = 1\n",
    "clip = 1\n",
    "\n",
    "# インスタンスの作成\n",
    "model = GRU(input_size, hidden_size, output_size, dropout=dropout)\n",
    "model.initWeight(init_forget_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' LSTM '''\n",
    "hidden_size = 100\n",
    "dropout = 0.3\n",
    "lr = 0.01\n",
    "lr_decay = 1\n",
    "init_forget_bias = 1\n",
    "clip = 1\n",
    "\n",
    "# インスタンスの作成\n",
    "model = LSTM(input_size, hidden_size, output_size, dropout=dropout)\n",
    "model.initWeight(init_forget_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' SRU '''\n",
    "phi_size = 200\n",
    "r_size = 60\n",
    "cell_out_size = 200\n",
    "lr = 0.1\n",
    "lr_decay = 0.99\n",
    "clip = 1\n",
    "\n",
    "torch.cuda.manual_seed(0)\n",
    "model = SRU(input_size, phi_size, r_size, cell_out_size,\n",
    "            output_size, A=[0, 0.5, 0.9, 0.99, 0.999],\n",
    "            dropout=0.2)\n",
    "model.initWeight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' loss, optimizerの定義 '''\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr) # SRUはRMSprop等と相性が悪い?\n",
    "# 10ステップごとに学習率をdecayさせる\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:: 1, (0m 9s) train_cost: 0.099, valid_cost: 0.107, train_acc: 0.220, valid_acc: 0.218\n",
      "EPOCH:: 2, (0m 19s) train_cost: 0.087, valid_cost: 0.096, train_acc: 0.224, valid_acc: 0.220\n",
      "EPOCH:: 3, (0m 28s) train_cost: 0.069, valid_cost: 0.084, train_acc: 0.230, valid_acc: 0.228\n",
      "EPOCH:: 4, (0m 38s) train_cost: 0.097, valid_cost: 0.090, train_acc: 0.222, valid_acc: 0.224\n",
      "EPOCH:: 5, (0m 47s) train_cost: 0.067, valid_cost: 0.080, train_acc: 0.230, valid_acc: 0.229\n",
      "EPOCH:: 6, (0m 57s) train_cost: 0.143, valid_cost: 0.127, train_acc: 0.205, valid_acc: 0.211\n",
      "EPOCH:: 7, (1m 7s) train_cost: 0.102, valid_cost: 0.115, train_acc: 0.218, valid_acc: 0.214\n",
      "EPOCH:: 8, (1m 17s) train_cost: 0.080, valid_cost: 0.396, train_acc: 0.226, valid_acc: 0.129\n",
      "EPOCH:: 9, (1m 27s) train_cost: 0.103, valid_cost: 0.087, train_acc: 0.219, valid_acc: 0.224\n",
      "EPOCH:: 10, (1m 37s) train_cost: 0.064, valid_cost: 0.074, train_acc: 0.232, valid_acc: 0.229\n",
      "EPOCH:: 11, (1m 47s) train_cost: 0.064, valid_cost: 0.084, train_acc: 0.230, valid_acc: 0.225\n",
      "EPOCH:: 12, (1m 57s) train_cost: 0.101, valid_cost: 0.116, train_acc: 0.220, valid_acc: 0.215\n",
      "EPOCH:: 13, (2m 7s) train_cost: 0.077, valid_cost: 0.082, train_acc: 0.228, valid_acc: 0.224\n",
      "EPOCH:: 14, (2m 18s) train_cost: 0.078, valid_cost: 0.078, train_acc: 0.228, valid_acc: 0.228\n",
      "EPOCH:: 15, (2m 28s) train_cost: 0.067, valid_cost: 0.096, train_acc: 0.230, valid_acc: 0.224\n",
      "EPOCH:: 16, (2m 39s) train_cost: 0.068, valid_cost: 0.083, train_acc: 0.230, valid_acc: 0.228\n",
      "EPOCH:: 17, (2m 49s) train_cost: 0.060, valid_cost: 0.081, train_acc: 0.233, valid_acc: 0.227\n",
      "EPOCH:: 18, (2m 59s) train_cost: 0.086, valid_cost: 0.087, train_acc: 0.226, valid_acc: 0.225\n",
      "EPOCH:: 19, (3m 9s) train_cost: 0.076, valid_cost: 0.078, train_acc: 0.228, valid_acc: 0.228\n",
      "EPOCH:: 20, (3m 19s) train_cost: 0.061, valid_cost: 0.082, train_acc: 0.232, valid_acc: 0.225\n",
      "EPOCH:: 21, (3m 28s) train_cost: 0.063, valid_cost: 0.073, train_acc: 0.231, valid_acc: 0.229\n",
      "EPOCH:: 22, (3m 38s) train_cost: 0.078, valid_cost: 0.086, train_acc: 0.227, valid_acc: 0.225\n",
      "EPOCH:: 23, (3m 48s) train_cost: 0.060, valid_cost: 0.079, train_acc: 0.233, valid_acc: 0.226\n",
      "EPOCH:: 24, (3m 57s) train_cost: 0.065, valid_cost: 0.086, train_acc: 0.231, valid_acc: 0.225\n",
      "EPOCH:: 25, (4m 7s) train_cost: 0.058, valid_cost: 0.078, train_acc: 0.232, valid_acc: 0.226\n",
      "EPOCH:: 26, (4m 16s) train_cost: 0.093, valid_cost: 0.183, train_acc: 0.224, valid_acc: 0.191\n",
      "EPOCH:: 27, (4m 26s) train_cost: 0.144, valid_cost: 0.132, train_acc: 0.203, valid_acc: 0.208\n",
      "EPOCH:: 28, (4m 35s) train_cost: 0.110, valid_cost: 0.152, train_acc: 0.215, valid_acc: 0.202\n",
      "EPOCH:: 29, (4m 45s) train_cost: 0.105, valid_cost: 0.115, train_acc: 0.218, valid_acc: 0.212\n",
      "EPOCH:: 30, (4m 54s) train_cost: 0.095, valid_cost: 0.129, train_acc: 0.220, valid_acc: 0.208\n",
      "EPOCH:: 31, (5m 4s) train_cost: 0.092, valid_cost: 0.103, train_acc: 0.222, valid_acc: 0.219\n",
      "EPOCH:: 32, (5m 14s) train_cost: 0.083, valid_cost: 0.115, train_acc: 0.224, valid_acc: 0.217\n",
      "EPOCH:: 33, (5m 23s) train_cost: 0.090, valid_cost: 0.101, train_acc: 0.223, valid_acc: 0.221\n",
      "EPOCH:: 34, (5m 33s) train_cost: 0.072, valid_cost: 0.092, train_acc: 0.228, valid_acc: 0.221\n",
      "EPOCH:: 35, (5m 42s) train_cost: 0.071, valid_cost: 0.078, train_acc: 0.229, valid_acc: 0.228\n",
      "EPOCH:: 36, (5m 52s) train_cost: 0.064, valid_cost: 0.082, train_acc: 0.231, valid_acc: 0.226\n",
      "EPOCH:: 37, (6m 1s) train_cost: 0.079, valid_cost: 0.099, train_acc: 0.226, valid_acc: 0.221\n",
      "EPOCH:: 38, (6m 11s) train_cost: 0.068, valid_cost: 0.073, train_acc: 0.230, valid_acc: 0.228\n",
      "EPOCH:: 39, (6m 21s) train_cost: 0.058, valid_cost: 0.074, train_acc: 0.233, valid_acc: 0.229\n",
      "EPOCH:: 40, (6m 30s) train_cost: 0.056, valid_cost: 0.072, train_acc: 0.234, valid_acc: 0.229\n",
      "EPOCH:: 41, (6m 40s) train_cost: 0.057, valid_cost: 0.090, train_acc: 0.233, valid_acc: 0.222\n",
      "EPOCH:: 42, (6m 50s) train_cost: 0.065, valid_cost: 0.086, train_acc: 0.231, valid_acc: 0.226\n",
      "EPOCH:: 43, (6m 59s) train_cost: 0.069, valid_cost: 0.132, train_acc: 0.229, valid_acc: 0.210\n",
      "EPOCH:: 44, (7m 9s) train_cost: 0.073, valid_cost: 0.081, train_acc: 0.229, valid_acc: 0.226\n",
      "EPOCH:: 45, (7m 19s) train_cost: 0.062, valid_cost: 0.074, train_acc: 0.232, valid_acc: 0.228\n",
      "EPOCH:: 46, (7m 29s) train_cost: 0.061, valid_cost: 0.462, train_acc: 0.232, valid_acc: 0.133\n",
      "EPOCH:: 47, (7m 39s) train_cost: 0.145, valid_cost: 0.153, train_acc: 0.206, valid_acc: 0.196\n",
      "EPOCH:: 48, (7m 49s) train_cost: 0.120, valid_cost: 0.126, train_acc: 0.212, valid_acc: 0.208\n",
      "EPOCH:: 49, (7m 59s) train_cost: 0.090, valid_cost: 0.085, train_acc: 0.222, valid_acc: 0.227\n",
      "EPOCH:: 50, (8m 9s) train_cost: 0.070, valid_cost: 0.107, train_acc: 0.231, valid_acc: 0.220\n",
      "EPOCH:: 51, (8m 20s) train_cost: 0.114, valid_cost: 0.116, train_acc: 0.216, valid_acc: 0.216\n",
      "EPOCH:: 52, (8m 30s) train_cost: 0.092, valid_cost: 0.098, train_acc: 0.223, valid_acc: 0.223\n",
      "EPOCH:: 53, (8m 40s) train_cost: 0.072, valid_cost: 0.081, train_acc: 0.230, valid_acc: 0.226\n",
      "EPOCH:: 54, (8m 51s) train_cost: 0.067, valid_cost: 0.082, train_acc: 0.230, valid_acc: 0.226\n",
      "EPOCH:: 55, (9m 1s) train_cost: 0.056, valid_cost: 0.075, train_acc: 0.234, valid_acc: 0.228\n",
      "EPOCH:: 56, (9m 11s) train_cost: 0.065, valid_cost: 0.092, train_acc: 0.232, valid_acc: 0.225\n",
      "EPOCH:: 57, (9m 22s) train_cost: 0.060, valid_cost: 0.075, train_acc: 0.232, valid_acc: 0.227\n",
      "EPOCH:: 58, (9m 33s) train_cost: 0.058, valid_cost: 0.073, train_acc: 0.233, valid_acc: 0.228\n",
      "EPOCH:: 59, (9m 44s) train_cost: 0.059, valid_cost: 0.072, train_acc: 0.232, valid_acc: 0.229\n",
      "EPOCH:: 60, (9m 55s) train_cost: 0.058, valid_cost: 0.087, train_acc: 0.233, valid_acc: 0.224\n",
      "EPOCH:: 61, (10m 6s) train_cost: 0.068, valid_cost: 0.082, train_acc: 0.230, valid_acc: 0.226\n",
      "EPOCH:: 62, (10m 18s) train_cost: 0.078, valid_cost: 0.078, train_acc: 0.228, valid_acc: 0.227\n",
      "EPOCH:: 63, (10m 31s) train_cost: 0.133, valid_cost: 0.144, train_acc: 0.214, valid_acc: 0.208\n",
      "EPOCH:: 64, (10m 42s) train_cost: 0.080, valid_cost: 0.081, train_acc: 0.227, valid_acc: 0.227\n",
      "EPOCH:: 65, (10m 54s) train_cost: 0.059, valid_cost: 0.472, train_acc: 0.233, valid_acc: 0.147\n",
      "EPOCH:: 66, (11m 6s) train_cost: 0.122, valid_cost: 0.082, train_acc: 0.212, valid_acc: 0.226\n",
      "EPOCH:: 67, (11m 19s) train_cost: 0.064, valid_cost: 0.091, train_acc: 0.231, valid_acc: 0.224\n",
      "EPOCH:: 68, (11m 31s) train_cost: 0.070, valid_cost: 0.080, train_acc: 0.229, valid_acc: 0.229\n",
      "EPOCH:: 69, (11m 43s) train_cost: 0.054, valid_cost: 0.075, train_acc: 0.234, valid_acc: 0.230\n",
      "EPOCH:: 70, (11m 55s) train_cost: 0.065, valid_cost: 0.092, train_acc: 0.231, valid_acc: 0.224\n",
      "EPOCH:: 71, (12m 7s) train_cost: 0.055, valid_cost: 0.069, train_acc: 0.234, valid_acc: 0.231\n",
      "EPOCH:: 72, (12m 18s) train_cost: 0.056, valid_cost: 0.082, train_acc: 0.233, valid_acc: 0.225\n",
      "EPOCH:: 73, (12m 29s) train_cost: 0.060, valid_cost: 0.086, train_acc: 0.232, valid_acc: 0.225\n",
      "EPOCH:: 74, (12m 40s) train_cost: 0.059, valid_cost: 0.072, train_acc: 0.233, valid_acc: 0.228\n",
      "EPOCH:: 75, (12m 50s) train_cost: 0.057, valid_cost: 0.084, train_acc: 0.233, valid_acc: 0.227\n",
      "EPOCH:: 76, (13m 0s) train_cost: 0.170, valid_cost: 0.146, train_acc: 0.197, valid_acc: 0.204\n",
      "EPOCH:: 77, (13m 11s) train_cost: 0.110, valid_cost: 0.164, train_acc: 0.216, valid_acc: 0.196\n",
      "EPOCH:: 78, (13m 21s) train_cost: 0.109, valid_cost: 0.129, train_acc: 0.215, valid_acc: 0.211\n",
      "EPOCH:: 79, (13m 31s) train_cost: 0.094, valid_cost: 0.095, train_acc: 0.221, valid_acc: 0.222\n",
      "EPOCH:: 80, (13m 41s) train_cost: 0.091, valid_cost: 0.091, train_acc: 0.222, valid_acc: 0.222\n",
      "EPOCH:: 81, (13m 51s) train_cost: 0.089, valid_cost: 0.097, train_acc: 0.224, valid_acc: 0.224\n",
      "EPOCH:: 82, (14m 1s) train_cost: 0.062, valid_cost: 0.075, train_acc: 0.232, valid_acc: 0.228\n",
      "EPOCH:: 83, (14m 11s) train_cost: 0.067, valid_cost: 0.089, train_acc: 0.231, valid_acc: 0.221\n",
      "EPOCH:: 84, (14m 21s) train_cost: 0.074, valid_cost: 0.089, train_acc: 0.228, valid_acc: 0.223\n",
      "EPOCH:: 85, (14m 30s) train_cost: 0.059, valid_cost: 0.073, train_acc: 0.232, valid_acc: 0.228\n",
      "EPOCH:: 86, (14m 40s) train_cost: 0.086, valid_cost: 0.089, train_acc: 0.226, valid_acc: 0.224\n",
      "EPOCH:: 87, (14m 50s) train_cost: 0.059, valid_cost: 0.075, train_acc: 0.232, valid_acc: 0.227\n",
      "EPOCH:: 88, (15m 0s) train_cost: 0.055, valid_cost: 0.072, train_acc: 0.234, valid_acc: 0.228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:: 89, (15m 10s) train_cost: 0.056, valid_cost: 0.079, train_acc: 0.234, valid_acc: 0.228\n",
      "EPOCH:: 90, (15m 19s) train_cost: 0.054, valid_cost: 0.084, train_acc: 0.234, valid_acc: 0.225\n",
      "EPOCH:: 91, (15m 29s) train_cost: 0.059, valid_cost: 0.074, train_acc: 0.232, valid_acc: 0.229\n",
      "EPOCH:: 92, (15m 39s) train_cost: 0.064, valid_cost: 0.076, train_acc: 0.232, valid_acc: 0.229\n",
      "EPOCH:: 93, (15m 48s) train_cost: 0.054, valid_cost: 0.069, train_acc: 0.234, valid_acc: 0.232\n",
      "EPOCH:: 94, (15m 58s) train_cost: 0.089, valid_cost: 0.166, train_acc: 0.224, valid_acc: 0.192\n",
      "EPOCH:: 95, (16m 8s) train_cost: 0.102, valid_cost: 0.087, train_acc: 0.219, valid_acc: 0.224\n",
      "EPOCH:: 96, (16m 17s) train_cost: 0.059, valid_cost: 0.069, train_acc: 0.232, valid_acc: 0.228\n",
      "EPOCH:: 97, (16m 27s) train_cost: 0.076, valid_cost: 0.074, train_acc: 0.228, valid_acc: 0.229\n",
      "EPOCH:: 98, (16m 37s) train_cost: 0.051, valid_cost: 0.087, train_acc: 0.234, valid_acc: 0.226\n",
      "EPOCH:: 99, (16m 47s) train_cost: 0.067, valid_cost: 0.092, train_acc: 0.230, valid_acc: 0.222\n",
      "EPOCH:: 100, (16m 57s) train_cost: 0.061, valid_cost: 0.072, train_acc: 0.232, valid_acc: 0.230\n",
      "EPOCH:: 101, (17m 7s) train_cost: 0.052, valid_cost: 0.067, train_acc: 0.235, valid_acc: 0.231\n",
      "EPOCH:: 102, (17m 17s) train_cost: 0.065, valid_cost: 0.154, train_acc: 0.232, valid_acc: 0.204\n",
      "EPOCH:: 103, (17m 27s) train_cost: 0.067, valid_cost: 0.113, train_acc: 0.230, valid_acc: 0.217\n",
      "EPOCH:: 104, (17m 37s) train_cost: 0.114, valid_cost: 0.082, train_acc: 0.214, valid_acc: 0.225\n",
      "EPOCH:: 105, (17m 47s) train_cost: 0.059, valid_cost: 0.076, train_acc: 0.233, valid_acc: 0.229\n",
      "EPOCH:: 106, (17m 56s) train_cost: 0.072, valid_cost: 0.088, train_acc: 0.229, valid_acc: 0.223\n",
      "EPOCH:: 107, (18m 6s) train_cost: 0.065, valid_cost: 0.081, train_acc: 0.232, valid_acc: 0.226\n",
      "EPOCH:: 108, (18m 16s) train_cost: 0.073, valid_cost: 0.076, train_acc: 0.229, valid_acc: 0.229\n",
      "EPOCH:: 109, (18m 26s) train_cost: 0.057, valid_cost: 0.077, train_acc: 0.233, valid_acc: 0.227\n",
      "EPOCH:: 110, (18m 36s) train_cost: 0.051, valid_cost: 0.079, train_acc: 0.235, valid_acc: 0.228\n",
      "EPOCH:: 111, (18m 45s) train_cost: 0.049, valid_cost: 0.067, train_acc: 0.235, valid_acc: 0.232\n",
      "EPOCH:: 112, (18m 55s) train_cost: 0.052, valid_cost: 0.068, train_acc: 0.236, valid_acc: 0.229\n",
      "EPOCH:: 113, (19m 5s) train_cost: 0.055, valid_cost: 0.089, train_acc: 0.234, valid_acc: 0.224\n",
      "EPOCH:: 114, (19m 15s) train_cost: 0.078, valid_cost: 0.083, train_acc: 0.228, valid_acc: 0.224\n",
      "EPOCH:: 115, (19m 25s) train_cost: 0.064, valid_cost: 0.095, train_acc: 0.232, valid_acc: 0.222\n",
      "EPOCH:: 116, (19m 35s) train_cost: 0.061, valid_cost: 0.071, train_acc: 0.231, valid_acc: 0.231\n",
      "EPOCH:: 117, (19m 45s) train_cost: 0.052, valid_cost: 0.069, train_acc: 0.235, valid_acc: 0.231\n",
      "EPOCH:: 118, (19m 55s) train_cost: 0.070, valid_cost: 0.086, train_acc: 0.229, valid_acc: 0.223\n"
     ]
    }
   ],
   "source": [
    "''' 訓練 '''\n",
    "n_epochs = 200\n",
    "batch_size = 1024\n",
    "n_batches = train_X.shape[0]//batch_size\n",
    "n_batches_v = valid_X.shape[0]//batch_size\n",
    "all_acc = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_cost, valid_cost, train_acc, valid_acc  = 0, 0, 0, 0\n",
    "    train_X, train_y = shuffle(train_X, train_y, random_state=epoch)\n",
    "\n",
    "    # 訓練\n",
    "    model.train()\n",
    "    train_X_t = np.transpose(train_X, (1, 0, 2)) # X.shape => (seq_len, n_samples, n_features) に変換\n",
    "    for i in range(n_batches):\n",
    "        scheduler.step()\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        inputs, labels = train_X_t[:, start:end, :], train_y[start:end]\n",
    "        inputs, labels = Variable(torch.from_numpy(inputs).cuda()\n",
    "                         ), Variable(torch.from_numpy(labels).cuda())\n",
    "        cost, accuracy = train(model, inputs, labels, optimizer, criterion, clip)\n",
    "        train_cost += cost / n_batches\n",
    "        train_acc  += accuracy / n_batches\n",
    "\n",
    "    # 検証\n",
    "    model.eval()\n",
    "    valid_X_t = np.transpose(valid_X, (1, 0, 2)) # X.shape => (seq_len, n_samples, n_features) に変換\n",
    "    for i in range(n_batches_v):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        inputs, labels = valid_X_t[:, start:end, :], valid_y[start:end]\n",
    "        inputs, labels = Variable(torch.from_numpy(inputs).cuda()\n",
    "                         ), Variable(torch.from_numpy(labels).cuda())\n",
    "        cost, accuracy = validate(model, inputs, labels, optimizer, criterion)\n",
    "        valid_cost += cost / n_batches_v\n",
    "        valid_acc += accuracy / n_batches_v\n",
    "\n",
    "    all_acc.append(valid_acc)\n",
    "    print('EPOCH:: %i, (%s) train_cost: %.3f, valid_cost: %.3f, train_acc: %.3f, valid_acc: %.3f' % (epoch + 1,\n",
    "                       timeSince(start_time), train_cost, valid_cost, train_acc, valid_acc))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sand Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 236\n",
       "[torch.cuda.ByteTensor of size 1 (GPU 2)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' テスト '''\n",
    "test_X_t = np.transpose(test_X[5000:5500], (1, 0, 2))\n",
    "inputs, labels = test_X_t, test_y[5000:5500]\n",
    "inputs, labels = Variable(torch.from_numpy(inputs).cuda()\n",
    "                 ), Variable(torch.from_numpy(labels).cuda())\n",
    "model.initHidden(inputs.size(1))\n",
    "outputs = model(inputs)\n",
    "\n",
    "# 正解数\n",
    "(torch.max(outputs, 1)[1] == labels).sum()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' 勾配の確認 '''\n",
    "loss = criterion(outputs, labels)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-03 *\n",
       " -1.3640\n",
       " -3.9899\n",
       "  3.9046\n",
       " -0.2410\n",
       "  2.9663\n",
       " -7.2542\n",
       "  1.3611\n",
       "  3.0439\n",
       " -0.1566\n",
       "  1.7300\n",
       "[torch.cuda.FloatTensor of size 10 (GPU 2)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[5].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-1.6677e+00 -2.1592e+00  1.1355e+01  ...  -3.4879e+00 -1.7871e-01 -2.5049e+00\n",
       "-6.0168e+00  1.2347e+01 -1.9971e+00  ...  -9.8720e-01 -2.5509e+00 -2.5539e+00\n",
       "-4.8645e+00  1.1795e+01 -5.2879e+00  ...  -4.3604e-01 -6.9174e-01 -2.8169e+00\n",
       "                ...                   ⋱                   ...                \n",
       "-5.4314e+00 -9.9449e-01 -1.3796e+00  ...   1.0591e+01 -1.0177e+00  3.4657e-01\n",
       "-3.5827e-02 -4.4396e-01 -2.2040e+00  ...  -7.9230e+00 -1.8895e+00 -4.1059e+00\n",
       " 6.9418e-01 -6.3189e-01 -1.8002e+00  ...  -8.9616e+00 -1.7312e+00 -4.9029e+00\n",
       "[torch.cuda.FloatTensor of size 500x10 (GPU 2)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "outputsが全てのサンプルで同じになる理由: XWが0, bが≠0\n",
    "\"\"\"\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_py35",
   "language": "python",
   "name": "tf_py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
